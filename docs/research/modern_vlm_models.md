# Анализ VLM для обработки документов

:::info Ландшафт VLM 2026 года
:::

## Аннотация

В 2026 году область обработки документов (**Document AI**) переживает фундаментальную трансформацию, характеризующуюся переходом от фрагментированных конвейеров оптического распознавания символов (OCR) к унифицированным визуально-языковым моделям (**Vision-Language Models, VLM**).

Эта смена парадигмы, ускоренная появлением доступного высокопроизводительного оборудования в лице серии NVIDIA GeForce RTX 50 и созреванием механизмов инференса, таких как vLLM и SGLang, открывает возможности для развертывания систем промышленного уровня на локальных мощностях.
Данный отчет представляет собой исчерпывающее техническое исследование современных VLM, доступных на платформе Hugging Face по состоянию на начало 2026 года, с акцентом на их применимость для задач распознавания документов в условиях аппаратных ограничений видеокарты NVIDIA GeForce RTX 5060 Ti (16 ГБ VRAM). В работе проводится детальный архитектурный анализ трех доминирующих семейств моделей: **DeepSeek-OCR**, расширенного семейства **Qwen3-VL (2B, 4B, 8B)** и **OlmOCR**. Особое внимание уделяется стратегиям квантования (FP8, INT4), оптимизации KV-кэша и интеграции с фреймворками vLLM и SGLang.

## 1. Введение: Эволюция Document AI в эпоху генеративного интеллекта

Исторически задача извлечения информации из документов решалась посредством сложных, многоступенчатых конвейеров. Классический подход требовал наличия детектора текста (например, DBNet), модели распознавания текста (CRNN или SVTR) и отдельных модулей для анализа структуры (Layout Analysis) и извлечения таблиц. Такой подход, хотя и был эффективным для простых задач, страдал от накопления ошибок: если детектор пропускал область текста, распознаватель не имел шанса ее обработать, а структурный анализатор терял контекст.  
К 2025–2026 годам индустрия консолидировалась вокруг концепции VLM. В этой парадигме документ подается на вход модели как изображение, а на выходе генерируется структурированный текст (Markdown, JSON, HTML) в рамках единого прохода (end-to-end). Это позволило моделям не просто "читать" символы, но и "понимать" семантику: отличать заголовки от основного текста, интерпретировать сложные таблицы и даже решать задачи визуального рассуждения (Visual Question Answering) непосредственно над документом.  
Однако этот прогресс принес новые вызовы. VLM требуют значительных вычислительных ресурсов. Стандартные модели, такие как Qwen2-VL или InternVL, используют тысячи визуальных токенов для кодирования одной страницы высокого разрешения, что создает колоссальную нагрузку на видеопамять (VRAM) и пропускную способность памяти. Для инженеров и исследователей, работающих с потребительским оборудованием, таким как NVIDIA RTX 5060 Ti с 16 ГБ памяти, это создает дилемму: как балансировать между точностью (размером модели и разрешением) и производительностью (вмещаемостью в VRAM и скоростью инференса).  
Настоящий отчет призван решить эту дилемму, предоставляя глубокий анализ моделей и методов их оптимизации для указанной аппаратной конфигурации.

## 2. Аппаратный базис: Архитектура NVIDIA GeForce RTX 5060 Ti

Понимание аппаратных ограничений и возможностей является критическим первым шагом при выборе архитектуры нейросети. Видеокарта RTX 5060 Ti, выпущенная в апреле 2025 года, представляет собой уникальное сочетание характеристик, делающих ее входным билетом в мир локальных VLM.

### **2.1 Архитектура Blackwell и тензорные вычисления**

В основе RTX 5060 Ti лежит микроархитектура Blackwell (чип GB206), произведенная по техпроцессу TSMC 4N. Ключевым нововведением для задач ИИ являются тензорные ядра 5-го поколения. В отличие от предыдущей архитектуры Ada Lovelace, Blackwell оптимизирована для работы с низкими точностями, включая нативную поддержку формата FP8.  
Это имеет решающее значение для инференса больших языковых моделей. Использование FP8 позволяет теоретически удвоить пропускную способность вычислений и, что более важно для карт с ограниченной памятью, вдвое сократить объем памяти, необходимый для хранения весов модели и KV-кэша, по сравнению с форматом BF16/FP16. Для 16 ГБ VRAM это означает возможность запуска моделей, которые ранее требовали бы карт уровня RTX 3090 (24 ГБ).

### **2.2 Подсистема памяти: Революция GDDR7**

Для задач генерации текста (декодирования), которые являются авторегрессионными и ограничены пропускной способностью памяти (memory-bound), тип видеопамяти играет первостепенную роль. RTX 5060 Ti оснащена 16 ГБ памяти стандарта GDDR7.  
**Сравнительный анализ пропускной способности:**

* GDDR6 (RTX 4060 Ti): \~18 Гбит/с на контакт.  
* GDDR7 (RTX 5060 Ti): \~28 Гбит/с на контакт.

При 128-битной шине это обеспечивает пропускную способность порядка 448–500 ГБ/с. Хотя это меньше, чем у флагманских моделей с широкой шиной (384 бит), высокая частота GDDR7 компенсирует узость шины. Для VLM, где визуальный энкодер (compute-bound) сменяется языковым декодером (memory-bound), этот баланс позволяет поддерживать высокую скорость генерации токенов (Tokens Per Second, TPS).

### **2.3 Бюджетирование VRAM: Математика 16 Гигабайт**

Запуск VLM на 16 ГБ требует строгого планирования ресурсов. Память расходуется на три основных компонента:

1. **Веса модели (Model Weights):** Статический объем.  
2. **Активации (Activations):** Динамическая память для промежуточных вычислений, особенно велика во время обработки изображения (prefill).  
3. **KV-кэш (Key-Value Cache):** Память для хранения контекста внимания во время генерации.

В таблице ниже приведен обновленный расчет потребления памяти с учетом моделей Qwen3-VL 2B/4B.

Модель (Параметры) | Формат | Веса (ГБ) | Активации + Overhead (ГБ) | Оставшееся место под KV-кэш (из 16 ГБ) | Оценка пригодности |
:--- | :--- | :--- | :--- | :--- | :--- |
**Qwen3-VL-2B** | BF16 | ~4.2 | ~1.5 | **~10.3 ГБ** | **Идеально (High Batch)** |
**DeepSeek-OCR (3B)** | BF16 | ~6.0 | ~1.5 | ~8.5 ГБ | **Идеально** |
**Qwen3-VL-4B** | BF16 | ~8.5 | ~2.0 | **~5.5 ГБ** | **Отлично (Full Precision)** |
**Qwen3-VL-4B** | FP8 | ~4.8 | ~1.8 | **~9.4 ГБ** | **Идеально** |
**Qwen2.5-VL (7B)** | FP8 | ~7.5 | ~2.0 | ~6.5 ГБ | Отлично |
**Qwen3-VL (8B)** | FP8 | ~8.5 | ~2.2 | ~5.3 ГБ | Приемлемо |

*Таблица 1: Анализ бюджета видеопамяти. Qwen3-VL-4B в режиме FP8 или BF16 представляет собой "золотую середину", оставляя огромный запас памяти под контекст.*

## 3. Программная экосистема: vLLM и SGLang

Выбор модели неразрывно связан с выбором движка инференса. В 2026 году два фреймворка доминируют в сегменте высокопроизводительного локального запуска: **vLLM** и **SGLang**.

### **3.1 vLLM: Стандарт индустрии**

К 2026 году vLLM перешел на архитектуру V1, которая внесла критические улучшения для мультимодальных моделей. Поддержка vLLM является обязательным требованием в запросе, и это обосновано следующими факторами:

* **PagedAttention для визуальных токенов:** vLLM обрабатывает визуальные токены так же эффективно, как и текстовые, разбивая их на блоки. Это критично для таких моделей, как Qwen2.5-VL/Qwen3-VL, где количество визуальных токенов варьируется в зависимости от разрешения изображения. Без PagedAttention фрагментация памяти быстро приводила бы к ошибкам OOM на карте с 16 ГБ.  
* **Поддержка малых моделей Qwen3:** Начиная с версий vLLM конца 2025 года, добавлена нативная поддержка архитектур Qwen3-VL-2B и Qwen3-VL-4B, включая их специфические энкодеры изображений (Vision Encoders), которые отличаются от серии Qwen2.  
* **Поддержка DeepSeek-OCR:** В конце 2025 года vLLM официально интегрировал поддержку специализированной архитектуры DeepSeek-OCR, включая необходимые адаптеры логитов (Logits Processors) для предотвращения зацикливания генерации, свойственного OCR-задачам.

### **3.2 SGLang: Эффективность для сложных пайплайнов**

SGLang (Structured Generation Language) позиционируется как более специализированное решение, часто превосходящее vLLM в сценариях со сложной структурой промптов.

* **RadixAttention:** Ключевая технология SGLang, позволяющая кэшировать префиксы промптов. В задачах обработки документов мы часто используем один и тот же системный промпт (например, "Извлеки все таблицы из этого изображения в формате HTML...") для тысяч документов. RadixAttention сохраняет KV-кэш этого промпта, исключая необходимость его перевычисления для каждого нового документа.  
* **Архитектура E/P/D:** SGLang использует разделение на Encoder (кодирование изображений), Prefill (обработка промпта) и Decode (генерация). Для таких инструментов, как OlmOCR, которые полагаются на массивные анкерные промпты, SGLang обеспечивает существенный прирост производительности.

## 4. Кандидат №1: DeepSeek-OCR — Чемпион эффективности

**DeepSeek-OCR**, выпущенный в конце 2025 года, представляет собой специализированную модель, разработанную не как универсальный VLM, а как узкопрофильный инструмент для OCR. Это делает его, пожалуй, наиболее привлекательным кандидатом для запуска на RTX 5060 Ti для задач чистого распознавания текста.

### **4.1 Архитектурные инновации: Оптическая компрессия токенов**

Традиционные VLM (например, ранние версии InternVL) используют метод "тайлинга" (tiling): разрезают изображение высокого разрешения на множество квадратов (например, 448x448), кодируют каждый отдельно и склеивают результаты. Это приводит к генерации тысяч визуальных токенов (часто 2000–6000 на страницу), что резко замедляет фазу prefill и съедает память.  
DeepSeek-OCR использует принципиально иной подход: **Contextual Optical Compression**.

* **Энкодер:** Модель использует гибридный энкодер (на базе SAM-B и CLIP-L), который обучается сжимать визуальную информацию.  
* **Сжатие:** Вместо тысяч токенов, DeepSeek-OCR кодирует страницу всего в **100–400 токенов** (в зависимости от режима разрешения: Tiny, Small, Base, Large).  
* **Декодер:** Используется Mixture-of-Experts (MoE) архитектура с 3 млрд параметров (DeepSeekMoE-3B), где активными являются лишь \~570 млн параметров.

### **4.2 Преимущества для RTX 5060 Ti**

1. **Экономия памяти:** Малое количество визуальных токенов означает, что KV-кэш для изображения занимает в 10–20 раз меньше места, чем у конкурентов. На 16 ГБ VRAM это позволяет обрабатывать огромные пакеты документов (batch size) параллельно, что невозможно для стандартных VLM.  
2. **Скорость:** Благодаря малому числу токенов и MoE-декодеру, скорость генерации текста на 5060 Ti может достигать промышленных показателей (тысячи токенов в секунду).  
3. **Нативная точность:** Модель помещается в память в формате BF16, что исключает потери качества от квантования.

### **4.3 Специфика запуска в vLLM**

Для запуска DeepSeek-OCR в vLLM (версии 0.11.0+) требуется особая конфигурация, так как модель чувствительна к повторениям n-грамм. Необходимо использовать специализированный NGramPerReqLogitsProcessor.

```python
# Пример конфигурации аргументов для запуска (концептуальный код)
engine_args = AsyncEngineArgs(
    model="deepseek-ai/DeepSeek-OCR",
    trust_remote_code=True,
    gpu_memory_utilization=0.85, # Оставляем запас под активации
    max_model_len=8192,
    # Критически важно для предотвращения зацикливания:
    logits_processors=
)
```

## 5. Кандидат №2: Семейство Qwen3-VL (2B и 4B) — Идеальный баланс

Выпущенные в конце 2025 года модели **Qwen3-VL-2B** и **Qwen3-VL-4B** изменили правила игры для владельцев карт среднего сегмента. Это не просто "урезанные" версии старших моделей, а полноценные архитектуры, оптимизированные для эффективности.

### **5.1 Qwen3-VL-4B: Лучший выбор для 16 ГБ VRAM**

Модель с 4 миллиардами параметров попадает в "сладкое пятно" (sweet spot) для RTX 5060 Ti.

* **Потребление памяти:** В полном формате BF16 она занимает всего **\~8.5 ГБ**. Это означает, что вам **не нужно использовать квантование**, если вы этого не хотите. У вас остается почти 7.5 ГБ видеопамяти под KV-кэш, что позволяет загружать документы в очень высоком разрешении (Naive Dynamic Resolution) или обрабатывать несколько страниц параллельно.  
* **Thinking Mode:** Важнейшая особенность серии Qwen3-VL. Модель Qwen/Qwen3-VL-4B-Thinking обучена использовать цепочку рассуждений (Chain of Thought) перед выдачей ответа. Для сложных документов (финансовые отчеты, схемы) это позволяет 4B-модели достигать точности, сравнимой с 7B-8B моделями предыдущего поколения, просто за счет более "вдумчивого" анализа.

### **5.2 Qwen3-VL-2B: Скорость света**

Если ваша задача — простая оцифровка текста или потоковая обработка видеокадров, модель 2B не имеет конкурентов.

* **Скорость:** На RTX 5060 Ti она способна выдавать более 100 токенов в секунду.  
* **Memory Footprint:** Занимает менее 5 ГБ VRAM. Вы можете запустить 2 или даже 3 экземпляра этой модели на одной карте 5060 Ti (через vLLM server с tensor\_parallel\_size=1 на разных портах), чтобы обрабатывать параллельные потоки запросов.

### **5.3 Интеграция с vLLM**

Обе модели имеют первоклассную поддержку в vLLM.

* **Запуск:** vllm serve Qwen/Qwen3-VL-4B-Instruct \--dtype bfloat16 \--trust-remote-code  
* **FP8:** Доступны официальные квантованные версии (например, Qwen/Qwen3-VL-4B-Instruct-FP8), которые снижают потребление памяти весов до **\~4.8 ГБ**, превращая 5060 Ti в мощнейшую станцию обработки документов.

## 6. Кандидат №3: OlmOCR — Эталон верифицируемости

Если DeepSeek-OCR — это скорость, то **OlmOCR** — это надежность. Этот инструментарий, разработанный Allen Institute for AI (AI2) на базе Qwen2.5-VL-7B, представляет собой вершину точности для сложных PDF-документов.

### **6.1 Методология: Document Anchoring и RLVR**

OlmOCR решает проблему галлюцинаций (когда модель "придумывает" текст в пустых местах) двумя методами:

1. **Document Anchoring (Якорение):** В промпт модели подаются метаданные из PDF (координаты текстовых блоков, если они есть), что "привязывает" генерацию к реальному положению текста.  
2. **RLVR (Reinforcement Learning with Verifiable Rewards):** Модель дообучалась с использованием методов обучения с подкреплением, где наградой служило прохождение юнит-тестов (например, "соответствует ли количество строк в извлеченной таблице оригиналу?").

### **6.2 Запуск на RTX 5060 Ti**

Для запуска OlmOCR на карте с 16 ГБ необходимо использовать **FP8-версию** модели (allenai/olmOCR-2-7B-1025-FP8).

* **Бэкенд:** Разработчики настоятельно рекомендуют использовать **SGLang**. Причина в том, что "якорение" создает очень длинные системные промпты, насыщенные метаданными. SGLang с его RadixAttention обрабатывает такие промпты значительно эффективнее vLLM, снижая латентность.  
* **Производительность:** Хотя OlmOCR медленнее DeepSeek-OCR из\-за большего размера и количества визуальных токенов, он обеспечивает непревзойденное качество извлечения таблиц и формул.

## 7. Сравнительная таблица характеристик

Обновленная таблица с учетом моделей Qwen3 2B/4B.

Характеристика | DeepSeek-OCR (3B) | Qwen3-VL-4B (Thinking) | Qwen3-VL-2B (Instruct) | OlmOCR (Qwen2.5-VL-7B) |
:--- | :--- | :--- | :--- | :--- |
**Потребление VRAM** | ~6 ГБ (BF16) | ~8.5 ГБ (BF16) / ~5 ГБ (FP8) | ~4.2 ГБ (BF16) | ~7.5 ГБ (FP8) |
**Визуальные токены** | 100–400 (Сжатие) | 1000+ (Нативное) | 1000+ (Нативное) | 1000+ (Нативное) |
**Точность (Таблицы)** | Высокая | Очень высокая (Thinking) | Средняя | **Наивысшая** |
**Скорость** | **Очень высокая** | Высокая | **Экстремальная** | Средняя |
**Сильная сторона** | Массовая обработка | Баланс "Скорость/Ум" | Edge, видеопотоки | PDF, научные статьи |
**Поддержка vLLM** | Требует настройки | **Нативная** | **Нативная** | Нативная |

*Таблица 2: Сводное сравнение ключевых кандидатов для RTX 5060 Ti.*

## **8. Руководство по развертыванию на RTX 5060 Ti**

### **8.1 Сценарий A: Запуск Qwen3-VL-4B (Универсальный вариант)**

Этот сценарий наиболее сбалансирован. Вы получаете умную модель, которая легко влезает в память и не требует сложной настройки.

1. **Установка:**
```bash  
   pip install vllm\>=0.11.0 transformers\>=4.48.0
```

2. Запуск сервера (vLLM):  
   Мы используем BF16 для максимальной точности, так как памяти достаточно.  
```bash  
   vllm serve Qwen/Qwen3-VL-4B-Instruct \\  
     \--dtype bfloat16 \\  
     \--gpu-memory-utilization 0.90 \\  
     \--max-model-len 16384 \\  
     \--trust-remote-code
```

   *Совет:* Если вам нужно анализировать схемы или графики, замените Instruct на Thinking в названии модели.

### **8.2 Сценарий B: Запуск DeepSeek-OCR (Максимальная пропускная способность)**

Идеально для обработки архивов из тысяч страниц.

1. **Запуск:**  
```bash  
   vllm serve deepseek-ai/DeepSeek-OCR \\  
     \--dtype bfloat16 \\  
     \--gpu-memory-utilization 0.90 \\  
     \--max-model-len 8192 \\  
     \--trust-remote-code
```

## **9. Заключение и стратегические рекомендации**

Для владельца NVIDIA RTX 5060 Ti с 16 ГБ видеопамяти ландшафт VLM 2026 года предлагает отличный выбор:

1. **Лучший выбор по умолчанию:** **Qwen3-VL-4B-Instruct**. Эта модель идеально укладывается в 16 ГБ VRAM без каких-либо компромиссов (в формате BF16). Она достаточно умная для сложных документов и достаточно быстрая для большинства задач. Наличие версии "Thinking" позволяет ей конкурировать с более крупными моделями в задачах анализа.  
2. **Для массовой оцифровки:** **DeepSeek-OCR (3B)**. Если ваша задача — прогнать миллион страниц сканов через GPU, архитектура оптического сжатия DeepSeek сэкономит вам часы работы и гигабайты памяти.  
3. **Для критически важных данных:** **OlmOCR (FP8)**. Если ошибка в извлеченной цифре недопустима (финансы, медицина), используйте OlmOCR с бэкендом SGLang.  
4. **Для экспериментов и скорости:** **Qwen3-VL-2B**. Самая легкая и быстрая модель. Отлично подходит для тестирования пайплайнов или работы в условиях, где GPU занят и другими задачами.